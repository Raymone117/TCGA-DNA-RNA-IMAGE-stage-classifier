{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:05:24.280320Z",
     "start_time": "2025-09-14T15:05:23.084466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, re, argparse, pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "def stage_from_folder(name: str):\n",
    "    name_l = name.lower()\n",
    "    if 'stage 1' in name_l: return 'I'\n",
    "    if 'stage 2' in name_l: return 'II'\n",
    "    if 'stage 3' in name_l: return 'III'\n",
    "    if 'stage 4' in name_l: return 'IV'\n",
    "    raise ValueError(f\"Cannot parse stage from folder: {name}\")\n",
    "\n",
    "def split_from_folder(name: str):\n",
    "    name_l = name.lower()\n",
    "    if 'train' in name_l: return 'train'\n",
    "    if 'test'  in name_l: return 'test'\n",
    "    return 'train'\n",
    "\n",
    "def parse_slide_id(filename: str):\n",
    "    base = os.path.basename(filename)\n",
    "    # take substring before first '.' as slide_id\n",
    "    if '.' in base:\n",
    "        return base.split('.')[0]\n",
    "    return os.path.splitext(base)[0]\n",
    "\n",
    "def main(args):\n",
    "    root = args.stage_root\n",
    "    folders = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root,d))]\n",
    "    rows = []\n",
    "    for fd in folders:\n",
    "        stage = stage_from_folder(fd)\n",
    "        split = split_from_folder(fd)\n",
    "        folder_path = os.path.join(root, fd)\n",
    "        imgs = []\n",
    "        for ext in ('*.png','*.jpg','*.jpeg'):\n",
    "            imgs += glob(os.path.join(folder_path, ext))\n",
    "        for p in imgs:\n",
    "            slide = parse_slide_id(p)\n",
    "            rows.append({'patient_id': slide, 'path': os.path.abspath(p), 'wsi_id': slide, 'stage': stage, 'group': split})\n",
    "    if not rows:\n",
    "        print(\"No images found. Check your folder paths.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # write labels: unique patient entries (stage/group: prefer 'train' if mixed; otherwise first)\n",
    "    df_lab = df[['patient_id','stage','group']].drop_duplicates('patient_id')\n",
    "    # If a patient appears in both train/test folders, mark group as 'train' (you may want to fix your split)\n",
    "    dup = df_lab.duplicated('patient_id', keep=False)\n",
    "    if dup.any():\n",
    "        # keep the first occurrence; but warn\n",
    "        print(\"Warning: some patient_ids appear in multiple splits. Keeping the first occurrence.\")\n",
    "        df_lab = df_lab.drop_duplicates('patient_id', keep='first')\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    df_lab.to_csv(os.path.join(args.out_dir, 'labels.csv'), index=False)\n",
    "\n",
    "    # write separate indexes\n",
    "    df_train = df[df['group']=='train'][['patient_id','path','wsi_id']]\n",
    "    df_test  = df[df['group']=='test'][['patient_id','path','wsi_id']]\n",
    "    df_train.to_csv(os.path.join(args.out_dir, 'index_train.csv'), index=False)\n",
    "    df_test.to_csv(os.path.join(args.out_dir, 'index_test.csv'), index=False)\n",
    "\n",
    "    print(\"Wrote:\")\n",
    "    print(\" -\", os.path.join(args.out_dir, 'labels.csv'))\n",
    "    print(\" -\", os.path.join(args.out_dir, 'index_train.csv'))\n",
    "    print(\" -\", os.path.join(args.out_dir, 'index_test.csv'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class Args:\n",
    "        stage_root = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\"\n",
    "        out_dir = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\"\n",
    "    main(Args())"
   ],
   "id": "604883ab08d98782",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      " - C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\labels.csv\n",
      " - C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_train.csv\n",
      " - C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_test.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:07:51.068663Z",
     "start_time": "2025-09-14T15:07:51.044980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, argparse, json, pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "def main(args):\n",
    "    df = pd.read_csv(args.label_csv)\n",
    "    if args.task == 'A':\n",
    "        y = (df['stage'].astype(str).str.upper() == 'IV').astype(int).values\n",
    "        n_classes = 2\n",
    "    else:\n",
    "        mapping = {'I':0,'II':1,'III':2,'IV':3}\n",
    "        y = df['stage'].astype(str).str.upper().map(mapping).values\n",
    "        n_classes = 4\n",
    "    groups = df['patient_id'].values\n",
    "    sgkf = StratifiedGroupKFold(n_splits=args.n_splits, shuffle=True, random_state=42)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    for k, (tr, va) in enumerate(sgkf.split(X=df['patient_id'], y=y, groups=groups)):\n",
    "        tr_ids = df.iloc[tr]['patient_id'].tolist()\n",
    "        va_ids = df.iloc[va]['patient_id'].tolist()\n",
    "        split = {'train': tr_ids, 'val': va_ids, 'n_classes': n_classes, 'task': args.task}\n",
    "        out_dir = os.path.join(args.out_dir, f\"fold_{k}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        with open(os.path.join(out_dir, 'split.json'), 'w') as f:\n",
    "            json.dump(split, f, indent=2)\n",
    "    print(\"Saved splits to\", args.out_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class Args:\n",
    "        label_csv = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\labels.csv\"\n",
    "        task = \"B\"  # ← 二分类任务\n",
    "        n_splits = 5\n",
    "        out_dir = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\"\n",
    "\n",
    "    args = Args()\n",
    "    main(args)"
   ],
   "id": "847228cda7f897db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved splits to C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:15:13.247028Z",
     "start_time": "2025-09-14T15:15:12.943962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_path = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_train.csv\"\n",
    "test_path  = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_test.csv\"\n",
    "output_path = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_all.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "all_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "all_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ integration completed，saved as index_all.csv\")"
   ],
   "id": "625aea33c532a610",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 合并完成，已保存为 index_all.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:11:45.636783Z",
     "start_time": "2025-09-14T15:11:45.507176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# 1. labels.csv\n",
    "labels = pd.read_csv(r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\labels.csv\")\n",
    "stage2label = {\"I\": 0, \"II\": 1, \"III\": 2, \"IV\": 3}\n",
    "labels[\"label\"] = labels[\"stage\"].map(stage2label)\n",
    "labels = labels[[\"patient_id\", \"label\"]]\n",
    "labels.to_csv(\"labels_cleaned.csv\", index=False)\n",
    "\n",
    "# 2. index_train.csv\n",
    "train = pd.read_csv(r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_train.csv\")\n",
    "train_unique = train[[\"wsi_id\"]].drop_duplicates()\n",
    "train_unique.columns = [\"patient_id\"]\n",
    "train_unique.to_csv(\"index_train_cleaned.csv\", index=False)\n",
    "\n",
    "# 3. index_test.csv → index_val.csv\n",
    "test = pd.read_csv(r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_test.csv\")\n",
    "val_unique = test[[\"wsi_id\"]].drop_duplicates()\n",
    "val_unique.columns = [\"patient_id\"]\n",
    "val_unique.to_csv(\"index_val_cleaned.csv\", index=False)"
   ],
   "id": "34c4297c7b1bd43c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T16:19:51.755428Z",
     "start_time": "2025-09-14T15:15:45.221991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, argparse, h5py, numpy as np, torch, torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_model(backbone='resnet50'):\n",
    "    if backbone == 'resnet50':\n",
    "        model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        model.fc = torch.nn.Identity()\n",
    "        feat_dim = 2048\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model.eval()\n",
    "    return model, feat_dim\n",
    "\n",
    "def build_tfms(img_size=256):\n",
    "    from torchvision import transforms as T\n",
    "    tfms = T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    return tfms\n",
    "\n",
    "def walk_tiles_from_index(index_csv):\n",
    "    df = pd.read_csv(index_csv)\n",
    "    items = []\n",
    "    for _, row in df.iterrows():\n",
    "        pid = str(row['patient_id'])\n",
    "        path = str(row['path'])\n",
    "        wsi = str(row['wsi_id']) if 'wsi_id' in df.columns else '_flat'\n",
    "        items.append((pid, wsi, path))\n",
    "    return items\n",
    "\n",
    "def extract_features(index_csv, out_root, img_size=256, batch_size=256, backbone='resnet50'):\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, feat_dim = get_model(backbone)\n",
    "    model = model.to(device)\n",
    "    tfms = build_tfms(img_size)\n",
    "\n",
    "    items = walk_tiles_from_index(index_csv)\n",
    "    by_patient = {}\n",
    "    for pid, wsi, path in items:\n",
    "        by_patient.setdefault(pid, []).append((wsi, path))\n",
    "\n",
    "    for pid, lst in tqdm(by_patient.items(), desc='Extracting features'):\n",
    "        feats, metas = [], []\n",
    "        for i in range(0, len(lst), batch_size):\n",
    "            batch = lst[i:i+batch_size]\n",
    "            imgs = []\n",
    "            for wsi, path in batch:\n",
    "                try:\n",
    "                    img = Image.open(path).convert('RGB')\n",
    "                except Exception:\n",
    "                    continue\n",
    "                imgs.append(tfms(img))\n",
    "                metas.append((os.path.basename(path), wsi))\n",
    "            if not imgs:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                x = torch.stack(imgs, dim=0).to(device)\n",
    "                f = model(x).detach().cpu().numpy()\n",
    "            feats.append(f)\n",
    "        if not feats:\n",
    "            continue\n",
    "        feats = np.concatenate(feats, axis=0)\n",
    "        out_path = os.path.join(out_root, f\"{pid}.h5\")\n",
    "        with h5py.File(out_path, 'w') as h5:\n",
    "            h5.create_dataset('feat', data=feats, compression='gzip')\n",
    "            meta_arr = np.array(metas, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "            h5.create_dataset('meta', data=meta_arr)\n",
    "    print(\"Done. Saved to\", out_root)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_json = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\fold_0\\split.json\"\n",
    "    index_all_csv = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_all.csv\"\n",
    "    out_root = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\features\"\n",
    "\n",
    "    split = json.load(open(split_json))\n",
    "    ids_needed = set(split['train'] + split['val'])\n",
    "    index_all = pd.read_csv(index_all_csv)\n",
    "    index_needed = index_all[index_all['patient_id'].isin(ids_needed)]\n",
    "\n",
    "    tmp_index_csv = r\"C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\processed\\index_needed.csv\"\n",
    "    index_needed.to_csv(tmp_index_csv, index=False)\n",
    "\n",
    "    extract_features(tmp_index_csv, out_root)\n"
   ],
   "id": "9ae2a15f447918a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 71/71 [1:04:03<00:00, 54.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved to C:\\Users\\mxjli\\PyCharmMiscProject\\stage new\\features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T16:22:06.717256Z",
     "start_time": "2025-09-14T16:21:03.332264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==================== parameters setting ====================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_dir = r\"C:\\\\Users\\\\mxjli\\\\PyCharmMiscProject\\\\stage new\\\\features\"\n",
    "label_file = r\"C:\\\\Users\\\\mxjli\\\\PyCharmMiscProject\\\\stage new\\\\processed\\\\labels_cleaned.csv\"\n",
    "train_index_file = r\"C:\\\\Users\\\\mxjli\\\\PyCharmMiscProject\\\\stage new\\\\processed\\\\index_train_cleaned.csv\"\n",
    "val_index_file = r\"C:\\\\Users\\\\mxjli\\\\PyCharmMiscProject\\\\stage new\\\\processed\\\\index_val_cleaned.csv\"\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "max_epochs = 50\n",
    "early_stop_patience = 10\n",
    "lr = 1e-4\n",
    "\n",
    "# ==================== Dataset ====================\n",
    "class MILDataset(Dataset):\n",
    "    def __init__(self, index_csv, label_csv, feature_dir):\n",
    "        self.index_df = pd.read_csv(index_csv)\n",
    "        self.label_df = pd.read_csv(label_csv)\n",
    "        self.feature_dir = feature_dir\n",
    "\n",
    "        self.label_map = dict(zip(self.label_df[\"patient_id\"], self.label_df[\"label\"]))\n",
    "        self.patient_ids = self.index_df[\"patient_id\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.patient_ids[idx]\n",
    "        label = self.label_map[pid]\n",
    "\n",
    "        feature_path = os.path.join(self.feature_dir, f\"{pid}.h5\")\n",
    "        with h5py.File(feature_path, 'r') as f:\n",
    "            # 自动识别 key（取第一个 dataset）\n",
    "            feature_key = list(f.keys())[0]\n",
    "            features = f[feature_key][:]  # shape: [num_patches, feature_dim]\n",
    "\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# ==================== Gated Attention MIL ====================\n",
    "class GatedAttentionMIL(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=256, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.attention_V = nn.Linear(hidden_dim, 128)\n",
    "        self.attention_U = nn.Linear(hidden_dim, 128)\n",
    "        self.attention_weights = nn.Linear(128, 1)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = self.embedding(x)  # [N, H]\n",
    "        A_V = torch.tanh(self.attention_V(H))\n",
    "        A_U = torch.sigmoid(self.attention_U(H))\n",
    "        A = self.attention_weights(A_V * A_U)  # [N, 1]\n",
    "        A = torch.softmax(A, dim=0)  # MIL attention\n",
    "\n",
    "        M = torch.sum(A * H, dim=0)  # weighted sum\n",
    "        out = self.classifier(M)\n",
    "        return out, A\n",
    "\n",
    "# ==================== Train ====================\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    losses, all_preds, all_labels = [], [], []\n",
    "    for feats, labels in loader:\n",
    "        feats, labels = feats[0].to(device), labels.to(device)\n",
    "        outputs, _ = model(feats)\n",
    "        loss = criterion(outputs.unsqueeze(0), labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        all_preds.append(outputs.argmax().item())\n",
    "        all_labels.append(labels.item())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return np.mean(losses), f1\n",
    "\n",
    "# ==================== Validate ====================\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses, all_preds, all_labels, all_probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for feats, labels in loader:\n",
    "            feats, labels = feats[0].to(device), labels.to(device)\n",
    "            outputs, _ = model(feats)\n",
    "            loss = criterion(outputs.unsqueeze(0), labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            all_preds.append(outputs.argmax().item())\n",
    "            all_labels.append(labels.item())\n",
    "            all_probs.append(outputs.softmax(dim=-1).cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return np.mean(losses), f1, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# ==================== Run ====================\n",
    "train_dataset = MILDataset(train_index_file, label_file, feature_dir)\n",
    "val_dataset = MILDataset(val_index_file, label_file, feature_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = GatedAttentionMIL(input_dim=2048, hidden_dim=256, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_f1, patience = 0, 0\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_loss, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_f1, val_labels, val_probs = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, F1={train_f1:.4f} | Val Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "        # Save validation set prediction probabilities\n",
    "        df_out = pd.DataFrame(val_probs, columns=[f\"prob_{i}\" for i in range(num_classes)])\n",
    "        df_out.insert(0, \"label\", val_labels)\n",
    "        df_out.to_csv(\"val_preds.csv\", index=False)\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f\"Best validation F1: {best_f1:.4f}\")"
   ],
   "id": "c325f218952ae879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.3913, F1=0.1366 | Val Loss=1.4167, F1=0.1875\n",
      "Epoch 2: Train Loss=1.3365, F1=0.3749 | Val Loss=1.4200, F1=0.2667\n",
      "Epoch 3: Train Loss=1.3064, F1=0.3271 | Val Loss=1.4235, F1=0.1111\n",
      "Epoch 4: Train Loss=1.2684, F1=0.3820 | Val Loss=1.4340, F1=0.3631\n",
      "Epoch 5: Train Loss=1.2226, F1=0.4121 | Val Loss=1.4389, F1=0.2833\n",
      "Epoch 6: Train Loss=1.1883, F1=0.3802 | Val Loss=1.4441, F1=0.3631\n",
      "Epoch 7: Train Loss=1.1639, F1=0.4108 | Val Loss=1.4616, F1=0.2429\n",
      "Epoch 8: Train Loss=1.1151, F1=0.4425 | Val Loss=1.4535, F1=0.3631\n",
      "Epoch 9: Train Loss=1.0990, F1=0.5193 | Val Loss=1.4698, F1=0.1875\n",
      "Epoch 10: Train Loss=1.0656, F1=0.5037 | Val Loss=1.4589, F1=0.4583\n",
      "Epoch 11: Train Loss=1.0285, F1=0.5822 | Val Loss=1.4905, F1=0.2833\n",
      "Epoch 12: Train Loss=1.0192, F1=0.5469 | Val Loss=1.5076, F1=0.4583\n",
      "Epoch 13: Train Loss=0.9768, F1=0.5750 | Val Loss=1.5070, F1=0.2667\n",
      "Epoch 14: Train Loss=0.9209, F1=0.5861 | Val Loss=1.5659, F1=0.3000\n",
      "Epoch 15: Train Loss=0.8710, F1=0.6457 | Val Loss=1.5741, F1=0.2667\n",
      "Epoch 16: Train Loss=0.8349, F1=0.6653 | Val Loss=1.7188, F1=0.3000\n",
      "Epoch 17: Train Loss=0.7803, F1=0.7155 | Val Loss=1.6783, F1=0.4250\n",
      "Epoch 18: Train Loss=0.7362, F1=0.7333 | Val Loss=1.7333, F1=0.2833\n",
      "Epoch 19: Train Loss=0.6901, F1=0.7626 | Val Loss=1.9258, F1=0.4583\n",
      "Epoch 20: Train Loss=0.6308, F1=0.7700 | Val Loss=1.8217, F1=0.3500\n",
      "Early stopping triggered.\n",
      "Best validation F1: 0.4583\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
